import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torch.utils.data as Data
from torch.utils.data import DataLoader
import torchvision
import matplotlib.pyplot as plt
import numpy as np
import os
from skimage import io,transform
import skimage
import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"
#os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"



rock_1 = []
label_rock_1 = []
rock_2 = []
label_rock_2 = []
rock_3 = []
label_rock_3 = []
rock_4 = []
label_rock_4 = []
rock_5 = []
label_rock_5 = []
rock_6 = []
label_rock_6 = []
rock_7 = []
label_rock_7 = []

def get_files(file_dir):
    for file in os.listdir(file_dir + '/1'):
        rock_1.append(file_dir + '/1' + '/' + file)
        label_rock_1.append(1)
    for file in os.listdir(file_dir + '/2'):
        rock_2.append(file_dir + '/2' + '/' + file)
        label_rock_2.append(2)
    for file in os.listdir(file_dir + '/3'):
        rock_3.append(file_dir + '/3' + '/' + file)
        label_rock_3.append(3)
    for file in os.listdir(file_dir + '/4'):
        rock_4.append(file_dir + '/4' + '/' + file)
        label_rock_4.append(4)
    for file in os.listdir(file_dir + '/5'):
        rock_5.append(file_dir + '/5' + '/' + file)
        label_rock_5.append(5)
    for file in os.listdir(file_dir + '/6'):
        rock_6.append(file_dir + '/6' + '/' + file)
        label_rock_6.append(6)
    for file in os.listdir(file_dir + '/7'):
        rock_7.append(file_dir + '/7' + '/' + file)
        label_rock_7.append(7)
    image_list = np.hstack((rock_1, rock_2, rock_3, rock_4,rock_5,rock_6,rock_7))
    label_list = np.hstack((label_rock_1, label_rock_2, label_rock_3, label_rock_4,label_rock_5,label_rock_6,label_rock_7))
    temp = np.array([image_list, label_list])
    temp = temp.transpose()
    np.random.shuffle(temp)
    # 将所有的img和lab转换成list
    return temp

path_1 = 'G:/泰迪杯数据挖掘_data/data_deal/分割3'
temp = get_files(path_1)

BATCH_SIZE = 32
LR = 0.0004
print(len(temp))
temp_1 = temp[:17000]
temp_2 = temp[-1432:]

#训练数据
all_image_list_train = list(temp_1[:, 0])
all_label_list_train = list(temp_1[:, 1])
train_img = []
for i in all_image_list_train:
    img = skimage.io.imread(i)
    img = transform.resize(img,(64,64))
    img = img/255.0
    img = img.astype('float16')
    train_img.append(img)

all_label_list_train_1 = []
for j in all_label_list_train:
    all_label_list_train_1.append(int(j))

train_transform = transforms.Compose([
    transforms.Normalize((0.5,), (0.5,)), #將matrices转成 Tensor,並把数值normalize到[0,1](data normalization)
])
train_x = np.array(train_img)
train_y = np.array(all_label_list_train_1)
print(train_x.shape)
train_x_1 = train_x.reshape(17000,3,64,64)
train_x_1 = torch.from_numpy(train_x_1)

train_y_1 = torch.from_numpy(train_y)
torch_dataset = Data.TensorDataset(train_x_1,train_y_1)
# #测试数据
all_image_list_1 = list(temp_2[:, 0])
all_label_list_1 = list(temp_2[:, 1])
val_img = []
for i in all_image_list_1:
    img = skimage.io.imread(i)
    img = transform.resize(img, (64, 64))
    img = img/255.0
    img = img.astype('float16')
    val_img.append(img)
all_label_list_2 = []
for j in all_label_list_1:
    all_label_list_2.append(int(j))

train_loader = DataLoader(torch_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=0)

val_x = np.array(val_img)
val_y = np.array(all_label_list_2)
val_x=val_x.reshape(1432,3,64,64)
val_x=torch.from_numpy(val_x)
#  转换为torch张量
val_y=torch.from_numpy(val_y)

#测试集数据
cs_list_image = []
cs_name_1 = []
path_cs = 'G:/泰迪杯数据挖掘_data/B题测试数据/压缩'
for cs_name in os.listdir(path_cs):
    cs_name_1.append(cs_name)
    name_cs_total = path_cs+'/'+cs_name
    cs_list_image.append(name_cs_total)
cs_list_image_1 = []
for image_cs in cs_list_image:
    img = skimage.io.imread(image_cs)
    img = img / 255.0
    img = img.astype('float16')
    cs_list_image_1.append(img)
cs_list_image_1 = np.array(cs_list_image_1)
cs_list_image_1 = cs_list_image_1.reshape(35,3,64,64)
cs_list_image_1 = torch.from_numpy(cs_list_image_1)

class CNN(nn.Module):
    def __init__(self):
        super(CNN,self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(   #(1,28,28)(30,64,64,3)
                in_channels=3,     #1代表着灰度图片,,如果是3这个地方就是代表彩色图片
                out_channels=32,   #输出的特征值16个
                kernel_size=3,   #5x5卷积核
                stride=1,    # 步长,每次移动一个像素
                padding=1,   #扩充边缘,方便提取边缘特征  padding = (kernel_size-1)/2
            ),  #图片变成(16,28,28)(30,64,64,16)
            nn.Dropout(0.5),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),  #这个地方使用2x2的区域再一次卷积/  变成(32,14,14)(30,32,32,16)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(32,64,3,1,1),   #变成(32,14,14)(30,32,32,32)(16,32,5,1,2)
            nn.Dropout(0.25),
            nn.BatchNorm2d(64),
            nn.ReLU(),

            nn.MaxPool2d(2),   #变成(32,7,7)(30,16,16,96)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(64,32,3,1,1),
            nn.Dropout(0.25),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.conv4 = nn.Sequential(
            nn.Conv2d(32, 16, 3, 1, 1),
            nn.Dropout(0.25),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        # self.conv5 = nn.Sequential(
        #     nn.Conv2d(32,16,3,1,1),
        #     nn.Dropout(0.25),
        #     nn.BatchNorm2d(16),
        #     nn.ReLU(),
        #     nn.MaxPool2d(2),
        #
        # )
        # self.conv6 = nn.Sequential(
        #     nn.Conv2d(16, 16, 3, 1, 1),
        #     nn.BatchNorm2d(16),
        #     nn.ReLU(),
        #     nn.MaxPool2d(2),
        # )
        self.out = nn.Linear(16*4*4,8)#(96*16*16,)(32*7*7,10)

    def forward(self,x):   #进行展平
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        # x = self.conv5(x)
        #x = self.conv6(x)
        x = x.view(x.size(0),-1)   #(batch,32*7*7)
        output = self.out(x)
        return output

cnn = CNN()
acc_list = []
optimizer = torch.optim.Adam(cnn.parameters(),lr=LR)  #优化器
loss_fun = nn.CrossEntropyLoss()  #自带softmax
from sklearn.metrics import accuracy_score
train_losses = []
val_losses = []

EPOCH = 10
for epoch in range(EPOCH):
    for step,(b_x,b_y) in enumerate(train_loader):
        output_train = cnn(b_x)
        loss = loss_fun(output_train,b_y.long())
        #loss_val = loss_fun(output_val, y_val.long())
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_losses.append(loss)

        if step % 100 == 0:
            test_out = cnn(val_x)
            # cnn.eval()
            pre = torch.argmax(test_out,1)
            acc = accuracy_score(val_y,pre)
            acc_list.append(acc)
            print('Epoch:', epoch, '| train loss:%.4f' % loss.item(), '| test accuracy:%.4f' % acc)
#
# test_out_cs = cnn(cs_list_image_1)
# pred_cs = torch.argmax(test_out_cs,1)
# pred_cs_np = pred_cs.numpy()
# list_1 = []
# for k in pred_cs:
#     if k == 1:
#         z = '深灰色泥岩'
#         list_1.append(z)
#     if k == 2:
#         z = '黑色煤'
#         list_1.append(z)
#     if k == 3:
#         z = '灰色细砂岩'
#         list_1.append(z)
#     if k == 4:
#         z = '浅灰色细砂岩'
#         list_1.append(z)
#     if k == 5:
#         z = '深灰色粉砂质泥岩'
#         list_1.append(z)
#     if k == 6:
#         z = '灰黑色泥岩'
#         list_1.append(z)
#     if k == 7:
#         z = '灰色泥质粉砂岩'
#         list_1.append(z)
#
# print('测试的岩石类别:', pred_cs)
# print('测试岩石类别:',list_1)
# print('测试岩石的名字:',cs_name_1)
# import pandas as pd
#
# df_1 = pd.DataFrame(cs_name_1,columns=['name'])
# df_2 = pd.DataFrame(list_1,columns=['岩石类别_名字'])
# df_3 = pd.DataFrame(pred_cs,columns=['类别_数字'])
# df_4 = pd.concat([df_1,df_2,df_3],axis=1)
# df_4.to_csv(r'G:\泰迪杯数据挖掘_data\B题测试数据\result2.csv')

